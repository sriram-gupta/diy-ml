{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n",
      "28\n",
      " Total training data count 9595  uniquechars aka input vector 28 ,\n",
      "       X torch.Size([9595, 10, 28]) , y torch.Size([9595, 28]) \n",
      "       for Each x of seq of 10 characters , one y target character \n",
      "       Each  character is of 28 dimensional vector \n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# text=\"sriram kumar is a good boy. this is alpha beta gamma\"\n",
    "\n",
    "chunk_size = 10000\n",
    "# Read the text data from the file\n",
    "file_path = './books/harry_potter.txt'  # Update the path to your book file\n",
    "with open(file_path, 'r') as file:\n",
    "    book_text = file.read(chunk_size)\n",
    "\n",
    "# Convert the text to lowercase\n",
    "book_text_lower = book_text.lower()\n",
    "\n",
    "# You can optionally perform additional cleaning or preprocessing here if needed,\n",
    "# such as removing special characters, punctuation, or performing tokenization.\n",
    "\n",
    "# Example: Removing punctuation using regex\n",
    "import re\n",
    "text =  re.sub(r'[^a-zA-Z\\s]', '', book_text_lower) #re.sub(r'[^\\w\\s]', '', book_text_lower)\n",
    "\n",
    "# Now you have the cleaned text in lowercase. You can use book_text_cleaned for further processing.\n",
    "\n",
    "\n",
    "# Create Character Mappings \n",
    "chars = sorted(list(set(text)))\n",
    "n_chars = len(chars)\n",
    "char_to_index = {ch:i for i,ch in enumerate(chars)}\n",
    "index_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "print(char_to_index)\n",
    "print(n_chars)\n",
    "\n",
    "def one_hot_encode(char):\n",
    "    vector = np.zeros(n_chars)\n",
    "\n",
    "    vector[char_to_index[char]] = 1\n",
    "    return vector\n",
    "\n",
    "# Prepare input sequence and corresponding target characters \n",
    "seq_length = 10\n",
    "X_data, y_data = [] , []\n",
    "\n",
    "for i in range(0,len(text)-seq_length,1):\n",
    "    seq_in =  text[i:i+seq_length]\n",
    "    seq_out = text[i+seq_length]\n",
    "    X_data.append([one_hot_encode(char) for char in seq_in ])\n",
    "    y_data.append([one_hot_encode(seq_out)])\n",
    "\n",
    "# Convert data to pytorch tensors \n",
    "X = torch.tensor(X_data, dtype=torch.float32)\n",
    "# y = torch.tensor(np.array(y_data).squeeze())\n",
    "\n",
    "y = torch.tensor(np.array(y_data).squeeze(), dtype=torch.long)  # Change the data type to torch.long\n",
    "\n",
    "print( f\"\"\" Total training data count {X.shape[0]}  uniquechars aka input vector {y.shape[1]} ,\n",
    "       X {X.shape} , y {y.shape} \n",
    "       for Each x of seq of {X.shape[1]} characters , one y target character \n",
    "       Each  character is of {y.shape[1]} dimensional vector \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 28])\n",
      "['\\n', '\\n', 'w', 'h']\n",
      " hidden shape tensor([[ 0.0246, -0.0048, -0.0150, -0.1198, -0.2089,  0.0909,  0.1411, -0.2366,\n",
      "         -0.0254,  0.0137,  0.0608, -0.0524,  0.0273, -0.0580,  0.1160,  0.0740,\n",
      "          0.1266,  0.0246,  0.1072,  0.1110, -0.0395, -0.0713, -0.0220,  0.1484,\n",
      "          0.0124, -0.0821, -0.2595,  0.1276],\n",
      "        [ 0.0755,  0.0163, -0.0047, -0.2126, -0.2976,  0.0668,  0.1895, -0.2509,\n",
      "         -0.0398,  0.1024,  0.0931, -0.1112,  0.0048, -0.0547,  0.0726,  0.0544,\n",
      "          0.1044,  0.0051,  0.0590,  0.1670, -0.1144, -0.0211, -0.0870,  0.0977,\n",
      "          0.0566, -0.1103, -0.2327,  0.1807],\n",
      "        [ 0.1926,  0.0807, -0.0600, -0.1992, -0.4037, -0.0175,  0.1228, -0.2418,\n",
      "         -0.0737,  0.0659,  0.1062, -0.1004, -0.0020,  0.0242,  0.1004,  0.0890,\n",
      "          0.0151, -0.0237,  0.0435,  0.1044, -0.0525, -0.0218, -0.1134, -0.0197,\n",
      "         -0.0248, -0.1781, -0.2917,  0.1902],\n",
      "        [ 0.0236,  0.0232,  0.0340, -0.1386, -0.2281,  0.0480,  0.1940, -0.3688,\n",
      "         -0.1265, -0.0028,  0.0234, -0.1175, -0.0687, -0.0373,  0.0378,  0.0404,\n",
      "         -0.0143, -0.0550,  0.0375, -0.0287, -0.0964, -0.0424, -0.0455,  0.2040,\n",
      "          0.0780, -0.1528, -0.1894,  0.1395]], grad_fn=<AddmmBackward0>) \n",
      " hidden shape v \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(42) # For reproducibility\n",
    "hidden_size = 32\n",
    "# model = nn.Sequential(\n",
    "#     nn.RNN(n_chars, hidden_size),  # RNN layer\n",
    "#     nn.Linear(hidden_size, n_chars)  # Output layer mapping RNN hidden representation to character space\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "class CharGenerationRNN(nn.Module):\n",
    "    def __init__(self, n_chars, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(n_chars, hidden_size)\n",
    "        self.lin = nn.Linear(hidden_size, n_chars)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the RNN layer.\n",
    "        output, hidden = self.rnn(x)\n",
    "        \n",
    "        # Extract the output at the last time step.\n",
    "        last_output = output\n",
    "        \n",
    "        # Pass the last RNN output through the linear layer to get the final output.\n",
    "        return self.lin(last_output)\n",
    "\n",
    "\n",
    "\n",
    "model = CharGenerationRNN(n_chars,hidden_size)\n",
    "\n",
    "def sample_size_shape_test():\n",
    "    input = X[0][0:4]\n",
    "    \n",
    "    # print(f\"input {input} shape {input.shape} type {type(input)} \")\n",
    "    hidden_unit_ouput = model(input)\n",
    "    # print(output)\n",
    "    print(hidden_unit_ouput.shape)\n",
    "    print([ index_to_char[int(torch.argmax(one_hot_vector_char))] for one_hot_vector_char in input])\n",
    "    print(f\" hidden shape {hidden_unit_ouput} \")\n",
    "    print(f\" hidden shape {index_to_char[int(torch.argmax(hidden_unit_ouput[-1]))]} \")\n",
    "    # print(\"^ Last Hidden state is the ouput \")\n",
    "\n",
    "\n",
    "sample_size_shape_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9595, 10, 28]), torch.Size([9595, 28]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# print(model.state_dict())\n",
    "\n",
    "# Define loss and optimizer\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "\n",
    "\n",
    "\n",
    "X.shape , y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss : 1.808928370475769\n",
      "epoch 1 loss : 1.8046586513519287\n",
      "epoch 2 loss : 1.8007177114486694\n",
      "epoch 3 loss : 1.7978651523590088\n",
      "epoch 4 loss : 1.797671914100647\n",
      "epoch 5 loss : 1.799382209777832\n",
      "epoch 6 loss : 1.795212745666504\n",
      "epoch 7 loss : 1.79088294506073\n",
      "epoch 8 loss : 1.7889983654022217\n",
      "epoch 9 loss : 1.7864056825637817\n",
      "epoch 10 loss : 1.7854559421539307\n",
      "epoch 11 loss : 1.7872754335403442\n",
      "epoch 12 loss : 1.7874181270599365\n",
      "epoch 13 loss : 1.783036231994629\n",
      "epoch 14 loss : 1.7797266244888306\n",
      "epoch 15 loss : 1.7767844200134277\n",
      "epoch 16 loss : 1.7740968465805054\n",
      "epoch 17 loss : 1.774444341659546\n",
      "epoch 18 loss : 1.7760969400405884\n",
      "epoch 19 loss : 1.7732905149459839\n",
      "epoch 20 loss : 1.7695956230163574\n",
      "epoch 21 loss : 1.7678040266036987\n",
      "epoch 22 loss : 1.7644520998001099\n",
      "epoch 23 loss : 1.7616122961044312\n",
      "epoch 24 loss : 1.762022852897644\n",
      "epoch 25 loss : 1.7647000551223755\n",
      "epoch 26 loss : 1.765826940536499\n",
      "epoch 27 loss : 1.76207435131073\n",
      "epoch 28 loss : 1.7577074766159058\n",
      "epoch 29 loss : 1.7538152933120728\n",
      "epoch 30 loss : 1.7527879476547241\n",
      "epoch 31 loss : 1.7543340921401978\n",
      "epoch 32 loss : 1.7522759437561035\n",
      "epoch 33 loss : 1.7474743127822876\n",
      "epoch 34 loss : 1.7449619770050049\n",
      "epoch 35 loss : 1.7439205646514893\n",
      "epoch 36 loss : 1.7414677143096924\n",
      "epoch 37 loss : 1.7408177852630615\n",
      "epoch 38 loss : 1.7408479452133179\n",
      "epoch 39 loss : 1.7454677820205688\n",
      "epoch 40 loss : 1.7578314542770386\n",
      "epoch 41 loss : 1.7467862367630005\n",
      "epoch 42 loss : 1.739619255065918\n",
      "epoch 43 loss : 1.7410306930541992\n",
      "epoch 44 loss : 1.7394345998764038\n",
      "epoch 45 loss : 1.7319287061691284\n",
      "epoch 46 loss : 1.7353497743606567\n",
      "epoch 47 loss : 1.7334486246109009\n",
      "epoch 48 loss : 1.7311114072799683\n",
      "epoch 49 loss : 1.728346824645996\n",
      "epoch 50 loss : 1.7263567447662354\n",
      "epoch 51 loss : 1.723818302154541\n",
      "epoch 52 loss : 1.7254549264907837\n",
      "epoch 53 loss : 1.7234466075897217\n",
      "epoch 54 loss : 1.719327688217163\n",
      "epoch 55 loss : 1.717354416847229\n",
      "epoch 56 loss : 1.7154656648635864\n",
      "epoch 57 loss : 1.7164154052734375\n",
      "epoch 58 loss : 1.7184438705444336\n",
      "epoch 59 loss : 1.7166991233825684\n",
      "epoch 60 loss : 1.7190862894058228\n",
      "epoch 61 loss : 1.7279373407363892\n",
      "epoch 62 loss : 1.7280867099761963\n",
      "epoch 63 loss : 1.7170639038085938\n",
      "epoch 64 loss : 1.7130128145217896\n",
      "epoch 65 loss : 1.717286229133606\n",
      "epoch 66 loss : 1.7102090120315552\n",
      "epoch 67 loss : 1.7124907970428467\n",
      "epoch 68 loss : 1.7075620889663696\n",
      "epoch 69 loss : 1.706565499305725\n",
      "epoch 70 loss : 1.7101932764053345\n",
      "epoch 71 loss : 1.7081300020217896\n",
      "epoch 72 loss : 1.705583930015564\n",
      "epoch 73 loss : 1.7037169933319092\n",
      "epoch 74 loss : 1.699064016342163\n",
      "epoch 75 loss : 1.698583722114563\n",
      "epoch 76 loss : 1.698474645614624\n",
      "epoch 77 loss : 1.6950911283493042\n",
      "epoch 78 loss : 1.697438359260559\n",
      "epoch 79 loss : 1.6981817483901978\n",
      "epoch 80 loss : 1.6937636137008667\n",
      "epoch 81 loss : 1.6971148252487183\n",
      "epoch 82 loss : 1.6990724802017212\n",
      "epoch 83 loss : 1.6988439559936523\n",
      "epoch 84 loss : 1.6992741823196411\n",
      "epoch 85 loss : 1.6922807693481445\n",
      "epoch 86 loss : 1.688534140586853\n",
      "epoch 87 loss : 1.6911386251449585\n",
      "epoch 88 loss : 1.689134120941162\n",
      "epoch 89 loss : 1.6853529214859009\n",
      "epoch 90 loss : 1.6861141920089722\n",
      "epoch 91 loss : 1.686617374420166\n",
      "epoch 92 loss : 1.6911416053771973\n",
      "epoch 93 loss : 1.687054991722107\n",
      "epoch 94 loss : 1.6833186149597168\n",
      "epoch 95 loss : 1.6798361539840698\n",
      "epoch 96 loss : 1.684924602508545\n",
      "epoch 97 loss : 1.685525894165039\n",
      "epoch 98 loss : 1.6905308961868286\n",
      "epoch 99 loss : 1.6810165643692017\n"
     ]
    }
   ],
   "source": [
    "looses = []\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X)\n",
    "    # print(X.shape)\n",
    "    # print(\"------\")\n",
    "    y_pred = output[:,-1,:]\n",
    "    # print(y_pred.shape)\n",
    "    # print(\"------\")\n",
    "    # print(y.shape)\n",
    "    # print(\"------\")\n",
    "    loss = criteria(y_pred.float(),y.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    looses.append(loss.item())\n",
    "    if epoch%1 == 0:\n",
    "        print(f\"epoch {epoch} loss : {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 28])\n",
      "['\\n', '\\n', 'w', 'h']\n",
      " hidden shape tensor([[-1.2675, -1.7056,  1.7738,  2.1560,  1.9441,  1.6303, -2.2309,  1.9248,\n",
      "          1.8374,  2.5438,  0.7526,  0.4009, -0.8015,  0.1052,  1.3983, -0.5474,\n",
      "          1.2071,  1.1043, -1.0536, -1.2484,  1.7843,  2.4987, -0.9531, -0.6692,\n",
      "          2.2068, -2.1728, -3.5215, -1.3798],\n",
      "        [-0.9145, -1.9017,  2.9679,  0.1800,  0.6945, -2.3314, -1.9408,  3.4801,\n",
      "         -1.7673,  3.6643,  0.4927,  0.4204, -2.2974,  0.4703,  0.3772,  1.2529,\n",
      "          0.9932,  0.6764,  0.3370,  1.0761,  1.2819,  2.0112,  1.1288, -0.5162,\n",
      "          1.3360, -0.4055, -4.6875, -0.4544],\n",
      "        [ 0.2659,  1.3589,  2.2608, -0.8085,  0.2920, -3.9356,  0.3886,  1.2563,\n",
      "         -5.0885,  2.2399,  0.7852, -1.4836, -2.6735,  4.2599,  0.0565, -2.9677,\n",
      "          1.1422, -0.2652, -1.8120,  0.3384, -0.0619,  1.7933, -0.2971, -0.2966,\n",
      "          0.9539, -1.0676, -3.7285, -1.2522],\n",
      "        [-0.5125,  2.2139,  5.1599, -0.2908,  0.7290, -4.0583,  6.4634, -1.6868,\n",
      "         -4.0355, -0.7693,  3.8979, -4.2155, -2.7298,  1.2830,  0.9493, -1.1712,\n",
      "          4.6397, -2.4449, -5.4056,  2.8827,  1.3001,  2.6078,  1.3677, -2.1371,\n",
      "         -1.6871, -4.8398,  1.4423, -4.4108]], grad_fn=<AddmmBackward0>) \n",
      " hidden shape e \n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_size_shape_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14,  5, 21])\n",
      "har a bdedbbjjeddmdddemdmmdtmdttm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from time import sleep\n",
    "from IPython import display\n",
    "\n",
    "# Use Trained model to geenrate Data \n",
    "def String2Vec(string):\n",
    "    X_data=[]\n",
    "    X_data.append(([one_hot_encode(c) for idx,c in enumerate(string) ]))\n",
    "    return X_data\n",
    "\n",
    "def generativeAI(input_string,n_output_chars):\n",
    "    genText = input_string\n",
    "    # Convert data to pytorch tensors \n",
    "    X = torch.tensor(String2Vec(input_string), dtype=torch.float32).squeeze()\n",
    "\n",
    "    for i in range(n_output_chars):\n",
    "        hidden_unit_ouput = model(X)\n",
    "\n",
    "        # print(hidden_unit_ouput)\n",
    "        # Get the indices of the top 3 maximum values\n",
    "        topk_values, topk_indices = torch.topk(hidden_unit_ouput[:,-1].squeeze(), k=3)\n",
    "        print(topk_indices)\n",
    "        # Choose randomly from the top 3 indices\n",
    "        random_index = random.choice(topk_indices)\n",
    "        nextCharIndex = int(random_index)\n",
    "\n",
    "        # nextCharIndex = int(torch.argmax(hidden_unit_ouput))\n",
    "        nextChar = index_to_char[nextCharIndex]\n",
    "        nextCharOneHot = torch.tensor(one_hot_encode(nextChar), dtype=torch.float32).unsqueeze(dim=0)\n",
    "        # torch.hstack(X,one_hot_encode(index_to_char(nextChar)))\n",
    "        genText = genText+nextChar\n",
    "        print(genText)\n",
    "        sleep(0.5)\n",
    "        if i != n_output_chars-1:\n",
    "            display.clear_output(wait=True)\n",
    "        X = torch.vstack((X,nextCharOneHot))\n",
    "   \n",
    "    \n",
    "generativeAI('har',30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FROM CHATGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 1, 85]' is invalid for input of size 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     73\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(input_tensors)):\n\u001b[0;32m---> 74\u001b[0m         input_seq \u001b[39m=\u001b[39m input_tensors[i]\u001b[39m.\u001b[39;49mview(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39mlen\u001b[39;49m(char_to_index))\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m     75\u001b[0m         target_char \u001b[39m=\u001b[39m target_tensors[i]\n\u001b[1;32m     77\u001b[0m         \u001b[39m# Zero the gradients\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 1, 85]' is invalid for input of size 100"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the dataset from a text file\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "    return data\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(data):\n",
    "    unique_chars = list(set(data))\n",
    "    char_to_index = {char: i for i, char in enumerate(unique_chars)}\n",
    "    index_to_char = {i: char for i, char in enumerate(unique_chars)}\n",
    "    return char_to_index, index_to_char\n",
    "\n",
    "# Generate training pairs\n",
    "def generate_training_pairs(data, sequence_length=100):\n",
    "    input_sequences = []\n",
    "    target_chars = []\n",
    "    for i in range(0, len(data) - sequence_length, 1):\n",
    "        input_seq = data[i:i + sequence_length]\n",
    "        target_char = data[i + sequence_length]\n",
    "        input_sequences.append(input_seq)\n",
    "        target_chars.append(target_char)\n",
    "    return input_sequences, target_chars\n",
    "\n",
    "# Create the CharGenerationRNN model\n",
    "class CharGenerationRNN(nn.Module):\n",
    "    def __init__(self, n_chars, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(n_chars, hidden_size, batch_first=True)\n",
    "        self.lin = nn.Linear(hidden_size, n_chars)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, _ = self.rnn(x)\n",
    "        output = output[:, -1, :]  # Take the output from the last time step\n",
    "        output = self.lin(output)\n",
    "        return output\n",
    "\n",
    "# Training parameters\n",
    "file_path = './books/harry_potter.txt'  # Replace with the path to your text file\n",
    "sequence_length = 100\n",
    "hidden_size = 128\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "# Load and preprocess the data\n",
    "data = load_data(file_path)\n",
    "char_to_index, index_to_char = preprocess_data(data)\n",
    "\n",
    "# Generate training pairs\n",
    "input_sequences, target_chars = generate_training_pairs(data, sequence_length)\n",
    "\n",
    "# Convert characters to indices\n",
    "input_indices = [[char_to_index[char] for char in seq] for seq in input_sequences]\n",
    "target_indices = [char_to_index[char] for char in target_chars]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "input_tensors = torch.tensor(input_indices, dtype=torch.long)\n",
    "target_tensors = torch.tensor(target_indices, dtype=torch.long)\n",
    "\n",
    "# Create the model\n",
    "model = CharGenerationRNN(len(char_to_index), hidden_size)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(input_tensors)):\n",
    "        input_seq = input_tensors[i].view(-1, 1, len(char_to_index)).permute(1, 0, 2)\n",
    "        target_char = target_tensors[i]\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input_seq)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, target_char.unsqueeze(0))\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Generating text\n",
    "def generate_text(starting_sequence, model, char_to_index, index_to_char, max_length=100):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        current_sequence = starting_sequence\n",
    "        for _ in range(max_length):\n",
    "            # Convert the current sequence to indices\n",
    "            sequence_indices = [char_to_index[char] for char in current_sequence]\n",
    "            input_tensor = torch.tensor(sequence_indices, dtype=torch.long).view(-1, 1, len(char_to_index)).permute(1, 0, 2)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(input_tensor)\n",
    "\n",
    "            # Get the predicted next character\n",
    "            _, predicted_index = torch.max(output, 1)\n",
    "            predicted_char = index_to_char[predicted_index.item()]\n",
    "\n",
    "            # Append the predicted character to the sequence\n",
    "            current_sequence += predicted_char\n",
    "\n",
    "        return current_sequence\n",
    "\n",
    "# Example usage\n",
    "starting_sequence = \"To be or not to b\"\n",
    "generated_text = generate_text(starting_sequence, model, char_to_index, index_to_char)\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
